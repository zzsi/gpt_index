{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6457769-dfaf-4241-ab32-dcf901dde902",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPT Keyword Table Index Comparisons\n",
    "\n",
    "Comparing GPTSimpleKeywordTableIndex, GPTRAKEKeywordTableIndex, GPTKeywordTableIndex.\n",
    "\n",
    "- GPTSimpleKeywordTableIndex - uses simple regex to extract keywords.\n",
    "- GPTRAKEKeywordTableIndex - uses RAKE to extract keywords.\n",
    "- GPTKeywordTableIndex - uses GPT to extract keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075080e5-c255-4a5c-9330-9da11532e1c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GPTSimpleKeywordTableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b367b7ef-6a7d-4aee-b174-dba6ec4d2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from gpt_index import GPTSimpleKeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8248fa-e0bd-494a-ad68-8192ccc87696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf513184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from gpt_index import (\n",
    "    GPTKeywordTableIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    LLMPredictor,\n",
    "    PromptHelper\n",
    ")\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "max_input_size = 4096\n",
    "num_output = 200\n",
    "num_chunk_overlap = 20\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# pipe = pipeline(\n",
    "#     \"text2text-generation\", model=model, tokenizer=tokenizer, # max_new_tokens=100\n",
    "# )\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer,\n",
    "    # max_tokens=num_output,\n",
    "    # max_new_tokens=100\n",
    "    max_new_tokens=num_output,\n",
    ")\n",
    "prompt_helper = PromptHelper(\n",
    "    max_input_size,\n",
    "    num_output,\n",
    "    num_chunk_overlap,\n",
    "    chunk_size_limit=512,\n",
    ")\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78a868cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cf947dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \t\t\n",
      "\n",
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before col...\n",
      "> Adding chunk: microcomputers, everything changed. Now you cou...\n",
      "> Adding chunk: Mistress, which featured an intelligent compute...\n",
      "> Adding chunk: At the time this bothered me, but now it seems ...\n",
      "> Adding chunk: book about something to help you learn it. The ...\n",
      "> Adding chunk: it had to be possible to make enough to survive...\n",
      "> Adding chunk: I'll give you something to read in a few days.\"...\n",
      "> Adding chunk: foundation that summer, but I still don't know ...\n",
      "> Adding chunk: they don't sit very still. So the traditional m...\n",
      "> Adding chunk: and my money was running out, so at the end of ...\n",
      "> Adding chunk: learned that it's better for technology compani...\n",
      "> Adding chunk: belonged to, seemed to be pretty rigorous. No d...\n",
      "> Adding chunk: out not to be controlled by the Romans. You can...\n",
      "> Adding chunk: now in grad school at Harvard. It seemed to me ...\n",
      "> Adding chunk: For some reason there was no bed frame or sheet...\n",
      "> Adding chunk: graduate student stipend, I needed that seed fu...\n",
      "> Adding chunk: 1996. It was just as well we waited a few month...\n",
      "> Adding chunk: use it to make their own stores. But anything t...\n",
      "> Adding chunk: just a handful of employees would have seemed a...\n",
      "> Adding chunk: shirts for 4 years. But I had done this to get ...\n",
      "> Adding chunk: I got back to New York I resumed my old life, e...\n",
      "> Adding chunk: MIT, and though he'd made a lot of money the la...\n",
      "> Adding chunk: new Lisp, whose parentheses I now wouldn't even...\n",
      "> Adding chunk: in New York, the only people allowed to publish...\n",
      "> Adding chunk: years I wrote lots of essays about all kinds of...\n",
      "> Adding chunk: waste their time is a great spur to the imagina...\n",
      "> Adding chunk: There were VC firms, which were organized compa...\n",
      "> Adding chunk: out of it, we'd at least get to practice being ...\n",
      "> Adding chunk: That had to be fair, because it was twice as go...\n",
      "> Adding chunk: So I changed the name to Hacker News and the to...\n",
      "> Adding chunk: than the boss.\" He meant it both descriptively ...\n",
      "> Adding chunk: so we decided we'd try to recruit Sam Altman. W...\n",
      "> Adding chunk: to see how the painting I was working on would ...\n",
      "> Adding chunk: approach. That wouldn't have been feasible at t...\n",
      "> Adding chunk: few people about Bel while I was working on it....\n",
      "> Adding chunk: the last sentence of it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Notes\n",
      "\n",
      "[1] My...\n",
      "> Adding chunk: is a refinement only New Yorkers would know or ...\n",
      "> Adding chunk: common, but the VCs' customs still reflected th...\n",
      "> Adding chunk: Another problem with HN was a bizarre edge case...\n",
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleKeywordTableIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53833655-0296-4bcb-b501-259b043d68b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What did the author do after his time at YC?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query keywords: ['tell', 'words', 'hope', 'put', 'title', \"you are asked to type in two different words: 'yes_three_to_get_yes_three'\", 'text', 'keywords', 'first', 'easily', 'talking', 'type:\\nthis question has 4 answers. in the following sections', 'placing', 'key', 'following', 'list', 'got', 'asked', 'find', 'next', 'keyword', 'extracted', '4', 'sections', 'questions', \"the keyword you're looking for\", 'word', 'exact', 'want', 'answers', 'ones', \"and 'no_three_to_get_no_three'. type in your exact keyword by placing a '@' next to it. this will give us a list of the keywords in the text.\\nso we have four keywords in yc who will tell us who they're talking about. let's say they've got a title. a word is always a word â€“ that's how many people know a key word 'hope'.\\nlet's say we've extracted from them a title with a long title. we could easily extract four different words and put them one at a time in this case. the first four words are the ones we want\", 'give', 'could', 'the other two are questions. if they could', 'say', 'different', 'four', 'people', 'yes_three_to_get_yes_three', 'let', 'case', 'yc', 'two', 'one', 'extract', 'long', 'many', 'time', 'us', 'already', 'looking', 'know', \"to find the one you've already extracted\", 'no_three_to_get_no_three', 'always', 'question', 'type']\n",
      "Extracted keywords: ['tell', 'got', 'next', 'want', 'could', 'people', 'case', 'yc', 'two', 'one', 'long', 'time', 'us', 'already', 'know']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total LLM token usage: 9216 tokens\n",
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What did the author do after his time at YC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79853260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b> Otherwise, see if you can do a good job using the original version, adding more context.\n",
       "\n",
       "The one bit that is a little different to traditional m.o. is that it doesn't give a clear direction on the direction or how it may be interpreted. This is not necessarily good, but it's more likely than not. The fact that you should use the following (in your own notes and later in in an answer): the direction you're supposed to stick to based on a person's history (the first time you put the topic in question in the context that the idea is the one to be considered) or to set your own own way to draw.\n",
       "\n",
       "We don't know for sure what happens next, but the idea of something that is supposed to be \"correct\" is a pretty interesting idea. Here is a good link to the Wikipedia article.\n",
       "\n",
       "- David D. Auerbach (2007)\n",
       "\n",
       "(via Wikimedia Commons) - You Can't Always</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c4446f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \t\t\n",
      "\n",
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before col...\n",
      "> Adding chunk: microcomputers, everything changed. Now you cou...\n",
      "> Adding chunk: Mistress, which featured an intelligent compute...\n",
      "> Adding chunk: At the time this bothered me, but now it seems ...\n",
      "> Adding chunk: book about something to help you learn it. The ...\n",
      "> Adding chunk: it had to be possible to make enough to survive...\n",
      "> Adding chunk: I'll give you something to read in a few days.\"...\n",
      "> Adding chunk: foundation that summer, but I still don't know ...\n",
      "> Adding chunk: they don't sit very still. So the traditional m...\n",
      "> Adding chunk: and my money was running out, so at the end of ...\n",
      "> Adding chunk: learned that it's better for technology compani...\n",
      "> Adding chunk: belonged to, seemed to be pretty rigorous. No d...\n",
      "> Adding chunk: out not to be controlled by the Romans. You can...\n",
      "> Adding chunk: now in grad school at Harvard. It seemed to me ...\n",
      "> Adding chunk: For some reason there was no bed frame or sheet...\n",
      "> Adding chunk: graduate student stipend, I needed that seed fu...\n",
      "> Adding chunk: 1996. It was just as well we waited a few month...\n",
      "> Adding chunk: use it to make their own stores. But anything t...\n",
      "> Adding chunk: just a handful of employees would have seemed a...\n",
      "> Adding chunk: shirts for 4 years. But I had done this to get ...\n",
      "> Adding chunk: I got back to New York I resumed my old life, e...\n",
      "> Adding chunk: MIT, and though he'd made a lot of money the la...\n",
      "> Adding chunk: new Lisp, whose parentheses I now wouldn't even...\n",
      "> Adding chunk: in New York, the only people allowed to publish...\n",
      "> Adding chunk: years I wrote lots of essays about all kinds of...\n",
      "> Adding chunk: waste their time is a great spur to the imagina...\n",
      "> Adding chunk: There were VC firms, which were organized compa...\n",
      "> Adding chunk: out of it, we'd at least get to practice being ...\n",
      "> Adding chunk: That had to be fair, because it was twice as go...\n",
      "> Adding chunk: So I changed the name to Hacker News and the to...\n",
      "> Adding chunk: than the boss.\" He meant it both descriptively ...\n",
      "> Adding chunk: so we decided we'd try to recruit Sam Altman. W...\n",
      "> Adding chunk: to see how the painting I was working on would ...\n",
      "> Adding chunk: approach. That wouldn't have been feasible at t...\n",
      "> Adding chunk: few people about Bel while I was working on it....\n",
      "> Adding chunk: the last sentence of it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Notes\n",
      "\n",
      "[1] My...\n",
      "> Adding chunk: is a refinement only New Yorkers would know or ...\n",
      "> Adding chunk: common, but the VCs' customs still reflected th...\n",
      "> Adding chunk: Another problem with HN was a bizarre edge case...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_documents] Total LLM token usage: 26955 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "> Starting query: What did the author do after his time at YC?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query keywords: ['words', 'select', 'prefixs', 'character', 'result', 'matter', 'expression', 'provide', \"add the 'prefix' if you have one of the many comma-separated 'prefixs' set to '\\\\u' (this doesn't matter if it's a string that has both a double quote and a regular expression). for a list of all of the available keyword sets\", 'text', \"'d'\", 'prefix', 'following', \"or 'i' separators. if you're just looking for a name\", 'u', 'search', 'use', 'list', 'regular', 'created', 'comma', 'shown', 'keyword', \"of the type '*'. try to search 'keyword sets' with the single 's'\", 'separated', 'full', 'finally', 'available', 'string', 'try', 'double', 'set', 'quote', \"select 'keypenis' and click on 'keyword sets'. the search result will be shown\", 'possible', 'click', \"then extract the following words from the text and add the '|' or '|' in a single colons. 'keypenis' will provide you with the full character set. finally\", 'if any', 'sets', 'then use \\'keyword sets\\'\" \\'keypenis\\' will be created that is a character set and will', 'name', 'keystrokes', 'one', 'extract', 'many', 'separators', 'add', 'colons', 'looking', 'keypenis', 'and it should list all possible keystrokes', 'single', 'type']\n",
      "Extracted keywords: ['words', 'character', 'provide', 'text', 'prefix', 'following', 'search', 'use', 'list', 'regular', 'comma', 'keyword', 'separated', 'full', 'finally', 'available', 'string', 'try', 'quote', 'possible', 'name', 'one', 'many', 'add', 'looking', 'single']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [query] Total LLM token usage: 9295 tokens\n",
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Otherwise, see if you can do a good job using the original version, adding more context.\n",
       "\n",
       "The one bit that is a little different to traditional m.o. is that it doesn't give a clear direction on the direction or how it may be interpreted. This is not necessarily good, but it's more likely than not. The fact that you should use the following (in your own notes and later in in an answer): the direction you're supposed to stick to based on a person's history (the first time you put the topic in question in the context that the idea is the one to be considered) or to set your own own way to draw.\n",
       "\n",
       "We don't know for sure what happens next, but the idea of something that is supposed to be \"correct\" is a pretty interesting idea. Here is a good link to the Wikipedia article.\n",
       "\n",
       "- David D. Auerbach (2007)\n",
       "\n",
       "(via Wikimedia Commons) - You Can't Always</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index2 = GPTKeywordTableIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "index2.query(\"What did the author do after his time at YC?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bcca18-b644-4393-ad29-6c5f0424fb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "The author went on to write essays and work on other projects, including a new version of the Arc programming language and Hacker News. He also started painting, but stopped after a few months. In 2015, he started working on a new Lisp programming language, which he finished in 2019. The author then moved to England in 2016 with his family and continued writing essays. In 2019, he finished Bel and wrote a bunch of essays on various topics.\n",
       "\n",
       "The author also worked on building online stores in 1995 after finishing ANSI Common Lisp. He ran the software on servers and let users control it by clicking on links, which was a new concept at the time. In 1996, he co-founded Viaweb with Robert Morris, which was later acquired by Yahoo in 1998. After leaving Yahoo, the author moved back to New York and started painting again. In 2000, he had the idea for a web application that would let people edit code on a server and host the resulting applications, which later became known as \"Reddit\".</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f9593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d24f9a20-48a6-4131-91b9-b01448c6ecb5",
   "metadata": {},
   "source": [
    "#### GPTRAKEKeywordTableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d3f293-e608-4b90-86aa-9bce666dbcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jerry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gpt_index import GPTRAKEKeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1da3b-8231-4da9-8026-4f95481c79df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTRAKEKeywordTableIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f13e5543-c6cb-4651-986c-ecde0f4bf789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What did the author do after his time at YC?\n",
      "Extracted keywords: []\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What did the author do after his time at YC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae01ac3-55fa-43a3-9b24-f733072d5f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Empty response</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cee6cf-92df-40d8-8dad-a40b792de96f",
   "metadata": {},
   "source": [
    "#### GPTKeywordTableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d59ef6-70b0-47bb-818d-7237a3b7de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTKeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f1c67-6d73-4f37-afcf-9e637002fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTKeywordTableIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4f686-6825-49cf-a113-d2fdd484de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a483514d-4ab5-489d-8b99-7250df491ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "After a few years, the author decided to step away from Y Combinator to focus on other projects, such as painting and writing essays. In 2013, he handed over control of Y Combinator to Sam Altman. The author's mother passed away in 2014, and after taking some time to grieve, he returned to writing essays and working on Lisp. He continued working on Lisp until 2019, when he finally completed the project.\n",
       "\n",
       "In 2015, the author decided to move to England with his family. They originally intended to only stay for a year, but ended up liking it so much that they remained there. The author wrote Bel while living in England. In 2019, he finally finished the project. After completing Bel, the author wrote a number of essays on various topics. He continued writing essays through 2020, but also started thinking about other things he could work on.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e21ee-587c-4d8b-871e-cb99b94e3778",
   "metadata": {},
   "source": [
    "## GPT Keyword Table Query Comparisons\n",
    "Compare mode={\"default\", \"simple\", \"rake\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029961a-ec22-42a1-90d6-f5892eb81e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build table with default GPTKeywordTableIndex\n",
    "from gpt_index import GPTKeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTKeywordTableIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75b31da-4788-4295-8642-07ac5c4f11a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "Extracted keywords: ['y combinator', 'combinator']\n",
      "> Querying with idx: 235042210695008001: of excluding them, because there were so many s...\n",
      "> Querying with idx: 7029274505691774319: it was like living in another country, and sinc...\n",
      "> Querying with idx: 1773317813360405038: browser, and then host the resulting applicatio...\n",
      "> Querying with idx: 3866067077574405334: person, and from those we picked 8 to fund. The...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "The author went on to write a book about his experiences at Y Combinator, and then moved to England. He started writing essays again and also began working on a new Lisp programming language. He also wrote an essay about how he chooses what to work on.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\", mode=\"default\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b713f4-adfc-46f7-a795-5b333e33d49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "Extracted keywords: ['combinator']\n",
      "> Querying with idx: 235042210695008001: of excluding them, because there were so many s...\n",
      "> Querying with idx: 7029274505691774319: it was like living in another country, and sinc...\n",
      "> Querying with idx: 1773317813360405038: browser, and then host the resulting applicatio...\n",
      "> Querying with idx: 3866067077574405334: person, and from those we picked 8 to fund. The...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "The author went on to write a book about his experiences at Y Combinator, and then moved to England. He started writing essays again and also began working on a new Lisp programming language. He also wrote an essay about how he chooses what to work on.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simple\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\", mode=\"simple\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e19ad9-3190-45e5-a28d-235c28296d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "Extracted keywords: ['combinator']\n",
      "> Querying with idx: 235042210695008001: of excluding them, because there were so many s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jerry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Querying with idx: 7029274505691774319: it was like living in another country, and sinc...\n",
      "> Querying with idx: 1773317813360405038: browser, and then host the resulting applicatio...\n",
      "> Querying with idx: 3866067077574405334: person, and from those we picked 8 to fund. The...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "The author went on to write a book about his experiences at Y Combinator, and then moved to England. He started writing essays again and also began working on a new Lisp programming language. He also wrote an essay about how he chooses what to work on.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rake\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\", mode=\"rake\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3224a3c9b1337f594ee748a62ad21c20d391b97db9fe444b3dd99d9f7ca421e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
